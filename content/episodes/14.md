---
title: Machine Learning Fundamentals, Part 1 with Shannon Wirtz
date: 2025-11-13T07:00:00.000Z
subtitle: Exploring the foundations of machine learning - from core concepts and terminology to different learning approaches and model types
description: Shannon Wirtz, product analyst at Angi, joins us to demystify machine learning fundamentals. We explore what ML actually means, how it differs from traditional programming, and dive deep into core concepts including models, features, training data, and the various types of learning approaches.
file: 014-machine-learning-fundamentals-part-1-with-shannon-wirtz.mp3
size: 78268694
duration: 4882
hash: 0b6be5c1f19860fce3ab2c3437d22ec3
---

Shannon Wirtz, product analyst at Angi, joins us to explore the foundations of machine learning - breaking down the terminology, concepts, and approaches that form the bedrock of modern ML systems.

We start by understanding what machine learning actually means in practice, how it differs from traditional rules-based programming, and where it fits within the broader landscape of AI and deep learning.
Shannon shares insights from his experience working with ML models in his professional work, from predicting customer behavior to classification tasks.

The conversation covers everything from the fundamental building blocks (models, features, training sets) to the different paradigms of learning - supervised, unsupervised, semi-supervised, self-supervised, and reinforcement learning.
We explore why generalization is critical, how bias and variance affect model performance, and why the "garbage in, garbage out" principle is so important in ML.

Topics include:

- What machine learning means and how it differs from traditional programming
- The relationship between AI, machine learning, and deep learning
- Core ML concepts: models, training sets, samples, instances, datasets
- Classification vs regression problems
- Parameters vs hyperparameters in model training
- Generalization: why models must work on unseen data
- Bias and variance: understanding overfitting and underfitting
- Learning paradigms: supervised, unsupervised, semi-supervised, self-supervised, reinforcement
- Online vs batch learning approaches
- Instance-based vs model-based learning
- Anomaly detection and change point detection
- Features and the "garbage in garbage out" principle
- The curse of dimensionality: why more features isn't always better
- Dimension reduction techniques including PCA
- Model families: linear/logistic regression, decision trees, k-means, SVMs

Shannon also shares practical examples from his work, including predicting tradesperson behavior, handling missing data, and the importance of understanding your data's context and history before training models.

Whether you're new to machine learning or looking to solidify your understanding of the fundamentals, this episode provides a comprehensive foundation for understanding how ML systems work and why certain approaches are chosen for different problems.

_This is Part 1 of a 2-part series. In [Part 2](https://compiledconversations.com/15/), we'll explore ensemble learning, neural networks, model training and evaluation, interpretation techniques, and practical learning resources._

**Show Links**

- [Shannon Wirtz on LinkedIn](https://www.linkedin.com/in/shannon-wirtz-a8387144/)
- [Overfitting and Underfitting](https://en.wikipedia.org/wiki/Overfitting)
- [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
- [Semi-supervised Learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)
- [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning)
- [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)
- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)
- [k-means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)
- [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)
- [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine)
- [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)
- [Robust Principal Component Analysis](https://en.wikipedia.org/wiki/Robust_principal_component_analysis)
- [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)
- [Spurious Correlations](https://tylervigen.com/spurious-correlations)
- [Kaggle](https://www.kaggle.com/)
